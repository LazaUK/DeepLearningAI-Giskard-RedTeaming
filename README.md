# Red Teaming LLM Applications

Andrew Ng and Giskard team released great course called ["Red Teaming LLM Applications"](https://learn.deeplearning.ai/courses/red-teaming-llm-applications) on DeepLearning.AI site.

I've followed on-screen instructions to re-create their practical Jupyter notebooks and then adapted the code to run against **Azure OpenAI** service, as it has slightly different syntax in comparison to the original OpenAI endpoints.

Additionally, references to **llama-index** classes were updated, to make the helper functions compatible with version **0.10.x**.

## Lesson 1: Overview of LLM Vulnerabilities
First lesson provides an overview of LLM vulnerabilities. It describes hypothetical scenarios, causes of observed behaviour and potential impact. Four main categories of described LLM vulnerabilities are:
- Bias and stereotypes;
- Sensitive information disclosure;
- Service disruption;
- Hallucinations.

## Lesson 2: Red Teaming LLMs
## Lesson 3: Red Teaming at Scale
## Lesson 4: Red Teaming LLMs with LLMs
## Lesson 5: A Full Red Teaming Assessment
